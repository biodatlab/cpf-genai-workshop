{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# สร้าง retriever\n",
    "import chromadb\n",
    "import os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.readers.file.docs import PDFReader\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from llama_index.core.schema import Document, TextNode\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"...\" # ใส่ OpenAI API key ที่นี้\n",
    "PDF_DATA_PATH=\"../assets/personal_data_protection_policy.pdf\"\n",
    "CHUNK_SIZE=1000\n",
    "CHUNK_OVERLAP=250\n",
    "OPENAI_TEMP=0\n",
    "CHROMA_PERSISTENCE_PATH=\"./chroma_db\"\n",
    "CHROMA_COLLECTION_NAME=\"test4543ss332\"\n",
    "RETRIEVER_TOP_K=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup LLM & Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
    "\n",
    "embeddings = embed_model.get_text_embedding(\n",
    "    \"Open AI new Embeddings models is great.\"\n",
    ")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# ลองใช้งาน LLM\n",
    "llm.complete(\"What is the capital of Thailand?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG from scratch\n",
    "เราสามารถทำ RAG ได้เองโดยใช้วิธีการดังนี้\n",
    "1. ตัดเอกสารออกเป็นส่วนๆ\n",
    "2. เปลี่ยนแต่ละส่วนให้เป็น เวกเตอร์\n",
    "3. ใช้ nearest neighbor เพื่อหาเอกสารที่ใกล้เคียงกับคำถาม\n",
    "4. สร้างคำตอบจากเอกสารที่เลือกโดยใช้ LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PDFReader()\n",
    "documents = reader.load_data(PDF_DATA_PATH)\n",
    "embedder = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "text_splitter = SentenceSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess documents\n",
    "def ingest_documents(documents: list[Document], text_splitter: SentenceSplitter, embedder: HuggingFaceEmbedding) -> TextNode:\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            # 1. แบ่งเอกสารเป็น chunk ขนาด CHUNK_SIZE และมีการซ้ำกัน CHUNK_OVERLAP\n",
    "            text_splitter,\n",
    "            # 2. สร้างเวกเตอร์จาก OpenAI Embedding\n",
    "            embedder,\n",
    "        ]\n",
    "    )\n",
    "    nodes = pipeline.run(documents=documents)\n",
    "    return nodes\n",
    "    \n",
    "# สร้าง retriever โดยใช้ KNN\n",
    "def retrieve_documents(\n",
    "    source_documents: list[TextNode],\n",
    "    query: str,\n",
    "    n: int = 5,\n",
    ") -> list[str]:\n",
    "    # embed query\n",
    "    query_embedding = embedder.get_text_embedding(query)\n",
    "    \n",
    "    # ใช้ KNN ในการค้นหาเอกสารที่ใกล้เคียงกับ query\n",
    "    search_space = [\n",
    "        node.embedding for node in source_documents\n",
    "    ]\n",
    "    knn = NearestNeighbors(n_neighbors=n)\n",
    "    knn.fit(search_space)\n",
    "    \n",
    "    # หาเอกสารที่ใกล้เคียงกับ query\n",
    "    query_embedding = knn.kneighbors([query_embedding], return_distance=False)\n",
    "        \n",
    "    return [\n",
    "        source_documents[idx].text for idx in query_embedding[0]\n",
    "    ]\n",
    "    \n",
    "# ใส่ section ของเอกสารที่ search จาก retriever ลงใน prompt ของเรา \n",
    "# พร้อมกับคำถามที่ต้องการตอบ\n",
    "def get_qa_prmopt(document: list[str], question: str)-> str:\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    sys: You are AI assistant your jobs is to answer the question based on the given document.\n",
    "    # Document\n",
    "    {document}\n",
    "\n",
    "    # Question\n",
    "    {question}\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ทดสอบ Ingest function\n",
    "source_documents = ingest_documents(documents, text_splitter, embedder)\n",
    "print(source_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ทดสอบ Retrieve function\n",
    "retrieved_documents = retrieve_documents(source_documents, \"การเก็บรวบรวมข้อมูลส่วนบุคคล มีแนวทางอย่างไรบ้าง?\")\n",
    "print(retrieved_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ลองดู Prompt\n",
    "prompt = get_qa_prmopt(retrieved_documents, \"การเก็บรวบรวมข้อมูลส่วนบุคคล มีแนวทางอย่างไรบ้าง?\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# สร้าง RAG pipeline\n",
    "def answer_from_document(\n",
    "    documents: list[Document],\n",
    "    question: str,\n",
    "    llm: OpenAI,\n",
    "    max_retrieve: int = 5,\n",
    ") -> str:\n",
    "    # 1. แบ่งเอกสารเป็น chunk ขนาด CHUNK_SIZE และมีการซ้ำกัน CHUNK_OVERLAP ก่อนจะ embed\n",
    "    ingested_documents = ingest_documents(documents, text_splitter, embedder)\n",
    "    # 2. ค้นหาเอกสารที่ใกล้เคียงกับ query\n",
    "    retrieved_documents = retrieve_documents(ingested_documents, question, n=max_retrieve)\n",
    "    # 3. สร้าง prompt จากเอกสารที่ค้นหาได้\n",
    "    prompt = get_qa_prmopt(retrieved_documents, question)\n",
    "    # 4. ใช้ LLM ในการตอบคำถาม\n",
    "    answer = llm.complete(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ทดสอบการทำงานของ RAG pipeline\n",
    "answer = answer_from_document(\n",
    "    documents=documents,\n",
    "    question=\"การเก็บรวบรวมข้อมูลส่วนบุคคล มีแนวทางอย่างไรบ้าง?\",\n",
    "    llm=llm,\n",
    "    max_retrieve=RETRIEVER_TOP_K,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ใช้งาน RAG โดย LlamaIndex และ Vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup LlamaIndex Embeddings and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = TokenTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "\n",
    "Settings.node_parser = splitter\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "Settings.chunk_size=CHUNK_SIZE\n",
    "Settings.chunk_overlap=CHUNK_OVERLAP\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## อ่าน PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PDFReader()\n",
    "documents = reader.load_data(PDF_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check documents\n",
    "docs = [doc.text for doc in documents]\n",
    "print(\"Document's length: \", len(docs))\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Chroma Vector Store and Ingest Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_retriever(documents) -> VectorStoreIndex:\n",
    "    db = chromadb.Client()\n",
    "    \n",
    "    # [OPTIONAL] delete if existing\n",
    "    try:\n",
    "        db.delete_collection(CHROMA_COLLECTION_NAME)\n",
    "    except ValueError:\n",
    "        print(\"[ChromaDB] Failed to delete collection.\")\n",
    "        \n",
    "    chroma_collection = db.create_collection(CHROMA_COLLECTION_NAME)\n",
    "    \n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    \n",
    "    # StorageContext is not ded like the other one\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    \n",
    "    index = VectorStoreIndex(nodes, storage_context=storage_context, show_progress=True)\n",
    "\n",
    "    return index\n",
    "\n",
    "vector_store_index = generate_retriever(documents) \n",
    "retriever = vector_store_index.as_retriever(similarity_top_k=RETRIEVER_TOP_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ทดสอบการทำงานของ retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for docs in the vDB\n",
    "res = retriever.retrieve(\"การเก็บรวบรวมข้อมูลส่วนบุคคล มีแนวทางอย่างไรบ้าง?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Retrieve {len(res)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ทดสอบ our RAG workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_store_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"การเก็บรวบรวมข้อมูลส่วนบุคคล มีแนวทางอย่างไรบ้าง?\"\n",
    "response = query_engine.query(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the prompt-template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ทดลองใช้  Template เพื่อให้เราได้คำตอบในรูปแบบที่เราต้องการ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT_TEMPLATE = \"\"\"\n",
    "# Task\n",
    "From given document answer the following question.\n",
    "\n",
    "# Instructions\n",
    "- Answer the following question based on the given document.\n",
    "- You need to cite the source of the answer from the document.\n",
    "\n",
    "# Document:\n",
    "{document}\n",
    "\n",
    "# Question:\n",
    "{question}\n",
    "\n",
    "# Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_rag(\n",
    "    retriever: BaseRetriever,\n",
    "    llm: OpenAI,\n",
    "    qa_prompt: PromptTemplate,\n",
    "    question: str,\n",
    ") -> str:\n",
    "    # Retrieve the chunks\n",
    "    chunks = retriever.retrieve(question)\n",
    "    response = llm.complete(\n",
    "        qa_prompt.format(\n",
    "            document=chunks,\n",
    "            question=question,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = get_answer_from_rag(\n",
    "    retriever=query_engine,\n",
    "    llm=llm,\n",
    "    qa_prompt=QUERY_PROMPT_TEMPLATE,\n",
    "    question=\"การเก็บรวบรวมข้อมูลส่วนบุคคล มีแนวทางอย่างไรบ้าง?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Answer: \", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
