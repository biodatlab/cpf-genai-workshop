{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install llama-index-core pymupdf llama-index chromadb sentence-transformers pydantic llama-index-llms-openai llama-index-embeddings-huggingface llama-index-vector-stores-chroma\n",
    "!pip install llama-index-readers-file\n",
    "!pip install llama-index-embeddings-azure-openai\n",
    "!pip install llama-index-llms-azure-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# สร้าง retriever\n",
    "import chromadb\n",
    "import os\n",
    "from llama_index.core import Settings\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "# นำเข้าไฟล์ .env \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "PDF_DATA_PATH=\"../data/personal_data_protection_policy.pdf\"\n",
    "EMBEDDING_MODEL=\"BAAI/bge-base-en-v1.5\"\n",
    "CHUNK_SIZE=1000\n",
    "CHUNK_OVERLAP=250\n",
    "OPENAI_TEMP=0\n",
    "CHROMA_PERSISTENCE_PATH=\"./chroma_db\"\n",
    "CHROMA_COLLECTION_NAME=\"test4543ss332\"\n",
    "RETRIEVER_TOP_K=5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup LLM & Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
    "\n",
    "embeddings = embed_model.get_text_embedding(\n",
    "    \"Open AI new Embeddings models is great.\"\n",
    ")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='The capital of Thailand is Bangkok.', additional_kwargs={}, raw=ChatCompletion(id='chatcmpl-A5aY3Ob5EOcm3JttdRTfQoOgaM9eX', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The capital of Thailand is Bangkok.', refusal=None, role='assistant', function_call=None, tool_calls=None))], created=1725895495, model='gpt-4o-2024-05-13', object='chat.completion', service_tier=None, system_fingerprint='fp_25624ae3a5', usage=CompletionUsage(completion_tokens=7, prompt_tokens=14, total_tokens=21)), logprobs=None, delta=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(model=\"gpt-4o\")\n",
    "\n",
    "# ลองใช้งาน LLM\n",
    "llm.complete(\"What is the capital of Thailand?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup LlamaIndex Embeddings and Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = TokenTextSplitter(\n",
    "    chunk_size=CHUNK_SIZE,\n",
    "    chunk_overlap=CHUNK_OVERLAP,\n",
    ")\n",
    "\n",
    "Settings.node_parser = splitter\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "Settings.chunk_size=CHUNK_SIZE\n",
    "Settings.chunk_overlap=CHUNK_OVERLAP\n",
    "\n",
    "Settings.llm = LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file.docs import PDFReader\n",
    "\n",
    "reader = PDFReader()\n",
    "\n",
    "documents = reader.load_data(PDF_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check documents\n",
    "docs = [doc.text for doc in documents]\n",
    "print(\"Document's length: \", len(docs))\n",
    "print(\"\\n\".join(docs))\n",
    "\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Chroma Vector Store and Ingest Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings:   0%|          | 0/17 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 17/17 [00:01<00:00,  9.94it/s]\n"
     ]
    }
   ],
   "source": [
    "def generate_retriever(documents) -> VectorStoreIndex:\n",
    "    db = chromadb.Client()\n",
    "    \n",
    "    # [OPTIONAL] delete if existing\n",
    "    try:\n",
    "        db.delete_collection(CHROMA_COLLECTION_NAME)\n",
    "    except ValueError:\n",
    "        print(\"[ChromaDB] Failed to delete collection.\")\n",
    "        \n",
    "    chroma_collection = db.create_collection(CHROMA_COLLECTION_NAME)\n",
    "    \n",
    "    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "    \n",
    "    # StorageContext is not ded like the other one\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    \n",
    "    index = VectorStoreIndex(nodes, storage_context=storage_context, show_progress=True)\n",
    "\n",
    "    return index\n",
    "\n",
    "vector_store_index = generate_retriever(documents) \n",
    "retriever = vector_store_index.as_retriever(similarity_top_k=RETRIEVER_TOP_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check for docs in the vDB\n",
    "res = retriever.retrieve(\"การเก็บรวบรวมข้อมูลส่วนบุคคล มีแนวทางอย่างไรบ้าง?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieve 5 chunks\n"
     ]
    }
   ],
   "source": [
    "print(f\"Retrieve {len(res)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing our RAG workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = vector_store_index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"การเก็บรวบรวมข้อมูลส่วนบุคคล มีแนวทางอย่างไรบ้าง?\"\n",
    "response = query_engine.query(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the prompt-template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ทดลองใช้  Template เพื่อให้เราได้คำตอบในรูปแบบที่เราต้องการ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT_TEMPLATE = \"\"\"\n",
    "# Task\n",
    "From given document answer the following question.\n",
    "\n",
    "# Instructions\n",
    "- Answer the following question based on the given document.\n",
    "- You need to cite the source of the answer from the document.\n",
    "\n",
    "# Document:\n",
    "{document}\n",
    "\n",
    "# Question:\n",
    "{question}\n",
    "\n",
    "# Answer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_from_rag(\n",
    "    retriever: BaseRetriever,\n",
    "    llm: OpenAI,\n",
    "    qa_prompt: PromptTemplate,\n",
    "    question: str,\n",
    ") -> str:\n",
    "    # Retrieve the chunks\n",
    "    chunks = retriever.retrieve(question)\n",
    "    response = llm.complete(\n",
    "        qa_prompt.format(\n",
    "            document=chunks,\n",
    "            question=question,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = get_answer_from_rag(\n",
    "    retriever=query_engine,\n",
    "    llm=LLM,\n",
    "    qa_prompt=QUERY_PROMPT_TEMPLATE,\n",
    "    question=\"การเก็บรวบรวมข้อมูลส่วนบุคคล มีแนวทางอย่างไรบ้าง?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  การเก็บรวบรวมข้อมูลส่วนบุคคลมีแนวทางดังนี้:\n",
      "\n",
      "1. เมื่อพ้นระยะเวลาการใช้งานตามวัตถุประสงค์ในการเก็บรวบรวมข้อมูลส่วนบุคคล\n",
      "2. เมื่อเกินความจำเป็น\n",
      "3. เมื่อเจ้าของข้อมูลส่วนบุคคลร้องขอ\n",
      "4. เมื่อเจ้าของข้อมูลได้ถอนความยินยอม\n",
      "\n",
      "เว้นแต่หน่วยงานหรือผู้รับผิดชอบจะสามารถเก็บรักษาข้อมูลส่วนบุคคลไว้ต่อไปตามที่กฎหมายแต่ละประเทศกำหนด\n",
      "\n",
      "(ที่มา: personal_data_protection_policy.pdf, หน้า 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Answer: \", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
